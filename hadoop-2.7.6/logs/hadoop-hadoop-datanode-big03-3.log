2018-11-23 11:58:35,663 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = class.novalocal/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.6
STARTUP_MSG:   classpath = /home/hadoop/hadoop-2.7.6/etc/hadoop:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jsch-0.1.54.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/hadoop-annotations-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/hadoop-auth-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6-tests.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/hadoop-nfs-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6-tests.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-registry-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-common-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-api-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-common-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-client-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6-tests.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.6.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://shv@git-wip-us.apache.org/repos/asf/hadoop.git -r 085099c66cf28be31604560c376fa282e69282b8; compiled by 'kshvachk' on 2018-04-18T01:33Z
STARTUP_MSG:   java = 1.8.0_191
************************************************************/
2018-11-23 11:58:35,673 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-11-23 11:58:36,249 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-11-23 11:58:36,299 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2018-11-23 11:58:36,299 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-11-23 11:58:36,303 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-11-23 11:58:36,304 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is class.novalocal
2018-11-23 11:58:36,309 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-11-23 11:58:36,325 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-11-23 11:58:36,327 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2018-11-23 11:58:36,327 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2018-11-23 11:58:36,390 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-11-23 11:58:36,396 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-11-23 11:58:36,406 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-11-23 11:58:36,411 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-11-23 11:58:36,412 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-11-23 11:58:36,413 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-11-23 11:58:36,413 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-11-23 11:58:36,422 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 34294
2018-11-23 11:58:36,422 INFO org.mortbay.log: jetty-6.1.26
2018-11-23 11:58:36,533 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:34294
2018-11-23 11:58:36,621 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-11-23 11:58:36,789 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2018-11-23 11:58:36,789 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-11-23 11:58:36,894 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2018-11-23 11:58:36,906 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-11-23 11:58:36,964 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-11-23 11:58:36,977 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-11-23 11:58:36,997 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-11-23 11:58:37,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2018-11-23 11:58:37,013 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-11-23 11:58:37,014 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-11-23 11:58:37,269 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2018-11-23 11:58:37,274 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-hadoop/dfs/data/in_use.lock acquired by nodename 1749@class.novalocal
2018-11-23 11:58:37,275 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-hadoop/dfs/data is not formatted for namespace 861984575. Formatting...
2018-11-23 11:58:37,275 INFO org.apache.hadoop.hdfs.server.common.Storage: Generated new storageID DS-350ab1f0-93e1-4ffe-b479-2d15f696b67a for directory /tmp/hadoop-hadoop/dfs/data
2018-11-23 11:58:37,301 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-686337982-127.0.1.1-1542941881430
2018-11-23 11:58:37,301 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /tmp/hadoop-hadoop/dfs/data/current/BP-686337982-127.0.1.1-1542941881430
2018-11-23 11:58:37,302 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /tmp/hadoop-hadoop/dfs/data/current/BP-686337982-127.0.1.1-1542941881430 is not formatted for BP-686337982-127.0.1.1-1542941881430. Formatting ...
2018-11-23 11:58:37,302 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-686337982-127.0.1.1-1542941881430 directory /tmp/hadoop-hadoop/dfs/data/current/BP-686337982-127.0.1.1-1542941881430/current
2018-11-23 11:58:37,304 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=861984575;bpid=BP-686337982-127.0.1.1-1542941881430;lv=-56;nsInfo=lv=-63;cid=CID-ef88cc01-9814-4cb8-8d6c-b1c13142fd69;nsid=861984575;c=0;bpid=BP-686337982-127.0.1.1-1542941881430;dnuuid=null
2018-11-23 11:58:37,306 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID 6bb65dfb-273f-4f08-8aa2-9a0597d71803
2018-11-23 11:58:37,333 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-350ab1f0-93e1-4ffe-b479-2d15f696b67a
2018-11-23 11:58:37,333 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /tmp/hadoop-hadoop/dfs/data/current, StorageType: DISK
2018-11-23 11:58:37,337 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-11-23 11:58:37,337 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-686337982-127.0.1.1-1542941881430
2018-11-23 11:58:37,338 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-686337982-127.0.1.1-1542941881430 on volume /tmp/hadoop-hadoop/dfs/data/current...
2018-11-23 11:58:37,345 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-686337982-127.0.1.1-1542941881430 on /tmp/hadoop-hadoop/dfs/data/current: 8ms
2018-11-23 11:58:37,346 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-686337982-127.0.1.1-1542941881430: 9ms
2018-11-23 11:58:37,346 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-686337982-127.0.1.1-1542941881430 on volume /tmp/hadoop-hadoop/dfs/data/current...
2018-11-23 11:58:37,347 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-686337982-127.0.1.1-1542941881430 on volume /tmp/hadoop-hadoop/dfs/data/current: 0ms
2018-11-23 11:58:37,347 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 1ms
2018-11-23 11:58:37,349 ERROR org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: dfs.datanode.directoryscan.throttle.limit.ms.per.sec set to value below 1 ms/sec. Assuming default value of 1000
2018-11-23 11:58:37,350 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1542960262350ms with interval of 21600000ms
2018-11-23 11:58:37,350 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-686337982-127.0.1.1-1542941881430 on volume /tmp/hadoop-hadoop/dfs/data
2018-11-23 11:58:37,351 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/tmp/hadoop-hadoop/dfs/data, DS-350ab1f0-93e1-4ffe-b479-2d15f696b67a): finished scanning block pool BP-686337982-127.0.1.1-1542941881430
2018-11-23 11:58:37,358 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-686337982-127.0.1.1-1542941881430 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2018-11-23 11:58:37,403 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-686337982-127.0.1.1-1542941881430 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2018-11-23 11:58:37,403 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-11-23 11:58:37,404 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/tmp/hadoop-hadoop/dfs/data, DS-350ab1f0-93e1-4ffe-b479-2d15f696b67a): no suitable block pools found to scan.  Waiting 1814399944 ms.
2018-11-23 11:58:37,472 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-686337982-127.0.1.1-1542941881430 (Datanode Uuid 6bb65dfb-273f-4f08-8aa2-9a0597d71803) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=1
2018-11-23 11:58:37,472 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-686337982-127.0.1.1-1542941881430 (Datanode Uuid 6bb65dfb-273f-4f08-8aa2-9a0597d71803) service to localhost/127.0.0.1:9000
2018-11-23 11:58:37,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x8168c551a5f,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 3 msec to generate and 49 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-23 11:58:37,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-686337982-127.0.1.1-1542941881430
2018-11-23 12:00:51,076 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-686337982-127.0.1.1-1542941881430:blk_1073741825_1001 src: /127.0.0.1:41738 dest: /127.0.0.1:50010
2018-11-23 12:00:51,160 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41738, dest: /127.0.0.1:50010, bytes: 1366, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1152739550_1, offset: 0, srvID: 6bb65dfb-273f-4f08-8aa2-9a0597d71803, blockid: BP-686337982-127.0.1.1-1542941881430:blk_1073741825_1001, duration: 54226786
2018-11-23 12:00:51,175 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-686337982-127.0.1.1-1542941881430:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-11-23 12:56:20,035 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-686337982-127.0.1.1-1542941881430:blk_1073741826_1002 src: /127.0.0.1:42044 dest: /127.0.0.1:50010
2018-11-23 12:56:20,077 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:42044, dest: /127.0.0.1:50010, bytes: 34886, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1243255950_1, offset: 0, srvID: 6bb65dfb-273f-4f08-8aa2-9a0597d71803, blockid: BP-686337982-127.0.1.1-1542941881430:blk_1073741826_1002, duration: 40622594
2018-11-23 12:56:20,077 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-686337982-127.0.1.1-1542941881430:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-11-23 13:12:10,584 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xc1a0de44eeb,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 1 msec to generate and 3 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-23 13:12:10,584 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-686337982-127.0.1.1-1542941881430
2018-11-23 14:06:53,703 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-686337982-127.0.1.1-1542941881430:blk_1073741827_1003 src: /127.0.0.1:42428 dest: /127.0.0.1:50010
2018-11-23 14:06:53,740 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:42428, dest: /127.0.0.1:50010, bytes: 526, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1239559117_14, offset: 0, srvID: 6bb65dfb-273f-4f08-8aa2-9a0597d71803, blockid: BP-686337982-127.0.1.1-1542941881430:blk_1073741827_1003, duration: 28797388
2018-11-23 14:06:53,741 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-686337982-127.0.1.1-1542941881430:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-11-23 14:13:30,597 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-686337982-127.0.1.1-1542941881430:blk_1073741828_1004 src: /127.0.0.1:42476 dest: /127.0.0.1:50010
2018-11-23 14:13:30,629 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:42476, dest: /127.0.0.1:50010, bytes: 526, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1558051501_14, offset: 0, srvID: 6bb65dfb-273f-4f08-8aa2-9a0597d71803, blockid: BP-686337982-127.0.1.1-1542941881430:blk_1073741828_1004, duration: 28432986
2018-11-23 14:13:30,631 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-686337982-127.0.1.1-1542941881430:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-11-23 17:04:22,358 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-686337982-127.0.1.1-1542941881430 Total blocks: 4, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-23 19:12:11,334 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1fbf5ef86311,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-23 19:12:11,334 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-686337982-127.0.1.1-1542941881430
2018-11-23 23:04:22,355 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-686337982-127.0.1.1-1542941881430 Total blocks: 4, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-24 01:12:12,110 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x3364b1a50515,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-24 01:12:12,110 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-686337982-127.0.1.1-1542941881430
2018-11-24 05:04:22,356 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-686337982-127.0.1.1-1542941881430 Total blocks: 4, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-24 07:12:12,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x470a04c9b251,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-24 07:12:12,897 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-686337982-127.0.1.1-1542941881430
2018-11-24 11:04:22,352 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-686337982-127.0.1.1-1542941881430 Total blocks: 4, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-24 13:12:10,669 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x5aaea466ea40,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 1 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-24 13:12:10,670 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-686337982-127.0.1.1-1542941881430
2018-11-24 17:04:22,351 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-686337982-127.0.1.1-1542941881430 Total blocks: 4, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-24 19:12:11,455 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x6e53f78b3fea,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-24 19:12:11,455 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-686337982-127.0.1.1-1542941881430
2018-11-24 23:04:22,352 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-686337982-127.0.1.1-1542941881430 Total blocks: 4, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-25 01:12:12,234 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x81f94a594b55,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-25 01:12:12,235 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-686337982-127.0.1.1-1542941881430
2018-11-25 05:04:22,351 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-686337982-127.0.1.1-1542941881430 Total blocks: 4, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-25 07:12:12,991 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x959e9bb79538,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-25 07:12:12,991 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-686337982-127.0.1.1-1542941881430
2018-11-25 11:04:22,351 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-686337982-127.0.1.1-1542941881430 Total blocks: 4, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-25 13:12:10,784 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xa9433c85b903,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-25 13:12:10,784 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-686337982-127.0.1.1-1542941881430
2018-11-25 17:04:22,352 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-686337982-127.0.1.1-1542941881430 Total blocks: 4, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-25 19:12:11,541 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xbce88df9b377,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-25 19:12:11,541 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-686337982-127.0.1.1-1542941881430
2018-11-25 23:04:22,351 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-686337982-127.0.1.1-1542941881430 Total blocks: 4, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-26 01:12:12,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xd08ddefe9ed3,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-26 01:12:12,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-686337982-127.0.1.1-1542941881430
2018-11-26 05:04:22,352 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-686337982-127.0.1.1-1542941881430 Total blocks: 4, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-26 07:12:12,977 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xe4332c3ae4fb,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-26 07:12:12,978 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-686337982-127.0.1.1-1542941881430
2018-11-26 11:04:22,351 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-686337982-127.0.1.1-1542941881430 Total blocks: 4, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-26 13:12:10,652 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xf7d7c5f9c1d9,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-26 13:12:10,652 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-686337982-127.0.1.1-1542941881430
2018-11-26 17:04:22,351 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-686337982-127.0.1.1-1542941881430 Total blocks: 4, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-26 19:12:11,393 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x10b7d1668cc19,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 0 msec to generate and 3 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-26 19:12:11,393 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-686337982-127.0.1.1-1542941881430
2018-11-26 23:04:22,351 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-686337982-127.0.1.1-1542941881430 Total blocks: 4, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-27 01:12:12,112 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x11f2265a0b950,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 1 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-27 01:12:12,112 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-686337982-127.0.1.1-1542941881430
2018-11-27 05:04:22,351 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-686337982-127.0.1.1-1542941881430 Total blocks: 4, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-27 07:12:12,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x132c7b20ffdc1,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-27 07:12:12,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-686337982-127.0.1.1-1542941881430
2018-11-27 11:04:22,351 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-686337982-127.0.1.1-1542941881430 Total blocks: 4, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-27 13:12:13,455 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1466cfe55bb48,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-27 13:12:13,455 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-686337982-127.0.1.1-1542941881430
2018-11-27 17:04:22,351 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-686337982-127.0.1.1-1542941881430 Total blocks: 4, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-27 19:12:11,112 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x15a1196dceadb,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 0 msec to generate and 5 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-27 19:12:11,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-686337982-127.0.1.1-1542941881430
2018-11-27 23:04:22,351 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-686337982-127.0.1.1-1542941881430 Total blocks: 4, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-28 01:12:11,786 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x16db6e37eb669,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-28 01:12:11,786 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-686337982-127.0.1.1-1542941881430
2018-11-28 05:04:22,351 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-686337982-127.0.1.1-1542941881430 Total blocks: 4, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-28 07:12:12,448 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1815c2f4f7238,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-28 07:12:12,448 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-686337982-127.0.1.1-1542941881430
2018-11-28 11:04:22,351 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-686337982-127.0.1.1-1542941881430 Total blocks: 4, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-28 13:12:13,065 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1950178630b04,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-28 13:12:13,065 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-686337982-127.0.1.1-1542941881430
2018-11-28 17:04:22,351 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-686337982-127.0.1.1-1542941881430 Total blocks: 4, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-28 19:12:10,771 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1a8a613ff377a,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 1 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-28 19:12:10,771 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-686337982-127.0.1.1-1542941881430
2018-11-28 23:04:22,351 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-686337982-127.0.1.1-1542941881430 Total blocks: 4, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-29 01:12:11,482 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1bc4b6294474c,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 0 msec to generate and 4 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-29 01:12:11,482 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-686337982-127.0.1.1-1542941881430
2018-11-29 05:04:22,351 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-686337982-127.0.1.1-1542941881430 Total blocks: 4, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-29 07:12:12,162 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1cff0af81b877,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-29 07:12:12,162 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-686337982-127.0.1.1-1542941881430
2018-11-29 11:04:22,351 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-686337982-127.0.1.1-1542941881430 Total blocks: 4, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-29 13:12:12,817 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1e395faf5866f,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-29 13:12:12,817 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-686337982-127.0.1.1-1542941881430
2018-11-29 14:56:28,236 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-11-29 14:56:28,238 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at class.novalocal/127.0.1.1
************************************************************/
2018-12-07 11:31:47,701 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = class.novalocal/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.6
STARTUP_MSG:   classpath = /home/hadoop/hadoop-2.7.6/etc/hadoop:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jsch-0.1.54.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/hadoop-annotations-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/hadoop-auth-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6-tests.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/hadoop-nfs-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6-tests.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-registry-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-common-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-api-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-common-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-client-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6-tests.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.6.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://shv@git-wip-us.apache.org/repos/asf/hadoop.git -r 085099c66cf28be31604560c376fa282e69282b8; compiled by 'kshvachk' on 2018-04-18T01:33Z
STARTUP_MSG:   java = 1.8.0_191
************************************************************/
2018-12-07 11:31:47,711 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-12-07 11:31:48,375 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-12-07 11:31:48,452 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2018-12-07 11:31:48,452 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-12-07 11:31:48,457 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-12-07 11:31:48,460 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is class.novalocal
2018-12-07 11:31:48,464 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-12-07 11:31:48,481 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-12-07 11:31:48,483 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2018-12-07 11:31:48,483 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2018-12-07 11:31:48,547 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-12-07 11:31:48,554 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-12-07 11:31:48,564 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-12-07 11:31:48,569 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-12-07 11:31:48,570 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-12-07 11:31:48,570 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-12-07 11:31:48,570 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-12-07 11:31:48,579 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 40591
2018-12-07 11:31:48,579 INFO org.mortbay.log: jetty-6.1.26
2018-12-07 11:31:48,693 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:40591
2018-12-07 11:31:48,786 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-12-07 11:31:49,002 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2018-12-07 11:31:49,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-12-07 11:31:49,091 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2018-12-07 11:31:49,103 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-12-07 11:31:49,187 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-12-07 11:31:49,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-12-07 11:31:49,220 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-12-07 11:31:49,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2018-12-07 11:31:49,256 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-12-07 11:31:49,256 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-12-07 11:31:50,309 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-12-07 11:31:51,309 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-12-07 11:31:52,310 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-12-07 11:31:53,311 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-12-07 11:31:54,313 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-12-07 11:31:55,314 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-12-07 11:31:56,314 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-12-07 11:31:57,315 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-12-07 11:31:58,315 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-12-07 11:31:59,318 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-12-07 11:31:59,319 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-12-07 11:32:05,321 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-12-07 11:32:06,322 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-12-07 11:32:07,323 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-12-07 11:32:08,324 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-12-07 11:32:09,325 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-12-07 11:32:10,326 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-12-07 11:32:11,327 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-12-07 11:32:12,328 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-12-07 11:32:13,330 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-12-07 11:32:14,330 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-12-07 11:32:14,331 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-12-07 11:32:20,332 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-12-07 11:32:21,333 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-12-07 11:32:22,334 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-12-07 11:32:23,335 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-12-07 11:32:24,336 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-12-07 11:32:25,337 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-12-07 11:32:26,338 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-12-07 11:32:27,339 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-12-07 11:32:28,342 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-12-07 11:32:29,343 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-12-07 11:32:29,346 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-12-07 11:32:35,348 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-12-07 11:32:36,349 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-12-07 11:32:37,350 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-12-07 11:32:38,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-12-07 11:32:39,352 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-12-07 11:32:40,284 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-12-07 11:32:40,287 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at class.novalocal/127.0.1.1
************************************************************/
2018-12-07 11:33:49,292 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = class.novalocal/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.6
STARTUP_MSG:   classpath = /home/hadoop/hadoop-2.7.6/etc/hadoop:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jsch-0.1.54.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/hadoop-annotations-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/hadoop-auth-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6-tests.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/common/hadoop-nfs-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6-tests.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-registry-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-common-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-api-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-common-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-client-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6-tests.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.6.jar:/home/hadoop/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.6.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://shv@git-wip-us.apache.org/repos/asf/hadoop.git -r 085099c66cf28be31604560c376fa282e69282b8; compiled by 'kshvachk' on 2018-04-18T01:33Z
STARTUP_MSG:   java = 1.8.0_191
************************************************************/
2018-12-07 11:33:49,300 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-12-07 11:33:49,897 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-12-07 11:33:49,948 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2018-12-07 11:33:49,948 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-12-07 11:33:49,952 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-12-07 11:33:49,953 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is class.novalocal
2018-12-07 11:33:49,958 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-12-07 11:33:49,976 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-12-07 11:33:49,978 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2018-12-07 11:33:49,978 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2018-12-07 11:33:50,042 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-12-07 11:33:50,048 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-12-07 11:33:50,059 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-12-07 11:33:50,064 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-12-07 11:33:50,065 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-12-07 11:33:50,065 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-12-07 11:33:50,065 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-12-07 11:33:50,075 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 38268
2018-12-07 11:33:50,075 INFO org.mortbay.log: jetty-6.1.26
2018-12-07 11:33:50,194 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:38268
2018-12-07 11:33:50,270 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-12-07 11:33:50,437 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2018-12-07 11:33:50,437 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-12-07 11:33:50,541 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2018-12-07 11:33:50,554 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-12-07 11:33:50,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-12-07 11:33:50,629 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-12-07 11:33:50,645 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-12-07 11:33:50,657 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2018-12-07 11:33:50,668 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-12-07 11:33:50,676 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-12-07 11:33:50,942 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2018-12-07 11:33:50,947 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-hadoop/dfs/data/in_use.lock acquired by nodename 3493@class.novalocal
2018-12-07 11:33:50,948 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-hadoop/dfs/data is not formatted for namespace 50956588. Formatting...
2018-12-07 11:33:50,948 INFO org.apache.hadoop.hdfs.server.common.Storage: Generated new storageID DS-b975f5f7-2328-4592-8c9b-a786d629f19c for directory /tmp/hadoop-hadoop/dfs/data
2018-12-07 11:33:51,033 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1745546438-127.0.1.1-1544149999758
2018-12-07 11:33:51,033 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /tmp/hadoop-hadoop/dfs/data/current/BP-1745546438-127.0.1.1-1544149999758
2018-12-07 11:33:51,034 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /tmp/hadoop-hadoop/dfs/data/current/BP-1745546438-127.0.1.1-1544149999758 is not formatted for BP-1745546438-127.0.1.1-1544149999758. Formatting ...
2018-12-07 11:33:51,034 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-1745546438-127.0.1.1-1544149999758 directory /tmp/hadoop-hadoop/dfs/data/current/BP-1745546438-127.0.1.1-1544149999758/current
2018-12-07 11:33:51,038 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=50956588;bpid=BP-1745546438-127.0.1.1-1544149999758;lv=-56;nsInfo=lv=-63;cid=CID-72bee6cc-5395-4739-9c44-9835a65b3bb3;nsid=50956588;c=0;bpid=BP-1745546438-127.0.1.1-1544149999758;dnuuid=null
2018-12-07 11:33:51,040 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID 04c48cfa-039b-4f57-89f2-3db9de8d6e4c
2018-12-07 11:33:51,073 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-b975f5f7-2328-4592-8c9b-a786d629f19c
2018-12-07 11:33:51,073 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /tmp/hadoop-hadoop/dfs/data/current, StorageType: DISK
2018-12-07 11:33:51,078 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-12-07 11:33:51,078 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1745546438-127.0.1.1-1544149999758
2018-12-07 11:33:51,079 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1745546438-127.0.1.1-1544149999758 on volume /tmp/hadoop-hadoop/dfs/data/current...
2018-12-07 11:33:51,091 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1745546438-127.0.1.1-1544149999758 on /tmp/hadoop-hadoop/dfs/data/current: 11ms
2018-12-07 11:33:51,091 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1745546438-127.0.1.1-1544149999758: 12ms
2018-12-07 11:33:51,092 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1745546438-127.0.1.1-1544149999758 on volume /tmp/hadoop-hadoop/dfs/data/current...
2018-12-07 11:33:51,092 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1745546438-127.0.1.1-1544149999758 on volume /tmp/hadoop-hadoop/dfs/data/current: 0ms
2018-12-07 11:33:51,092 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 2ms
2018-12-07 11:33:51,095 ERROR org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: dfs.datanode.directoryscan.throttle.limit.ms.per.sec set to value below 1 ms/sec. Assuming default value of 1000
2018-12-07 11:33:51,095 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1544159518095ms with interval of 21600000ms
2018-12-07 11:33:51,096 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-1745546438-127.0.1.1-1544149999758 on volume /tmp/hadoop-hadoop/dfs/data
2018-12-07 11:33:51,096 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/tmp/hadoop-hadoop/dfs/data, DS-b975f5f7-2328-4592-8c9b-a786d629f19c): finished scanning block pool BP-1745546438-127.0.1.1-1544149999758
2018-12-07 11:33:51,110 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1745546438-127.0.1.1-1544149999758 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2018-12-07 11:33:51,121 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/tmp/hadoop-hadoop/dfs/data, DS-b975f5f7-2328-4592-8c9b-a786d629f19c): no suitable block pools found to scan.  Waiting 1814399972 ms.
2018-12-07 11:33:51,157 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1745546438-127.0.1.1-1544149999758 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2018-12-07 11:33:51,158 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-12-07 11:33:51,243 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1745546438-127.0.1.1-1544149999758 (Datanode Uuid 04c48cfa-039b-4f57-89f2-3db9de8d6e4c) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=1
2018-12-07 11:33:51,243 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1745546438-127.0.1.1-1544149999758 (Datanode Uuid 04c48cfa-039b-4f57-89f2-3db9de8d6e4c) service to localhost/127.0.0.1:9000
2018-12-07 11:33:51,303 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1f59ff5966d,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 3 msec to generate and 57 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-12-07 11:33:51,304 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1745546438-127.0.1.1-1544149999758
2018-12-07 11:38:11,973 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1745546438-127.0.1.1-1544149999758:blk_1073741825_1001 src: /127.0.0.1:60398 dest: /127.0.0.1:50010
2018-12-07 11:38:12,048 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:60398, dest: /127.0.0.1:50010, bytes: 34886, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2041714848_1, offset: 0, srvID: 04c48cfa-039b-4f57-89f2-3db9de8d6e4c, blockid: BP-1745546438-127.0.1.1-1544149999758:blk_1073741825_1001, duration: 50069599
2018-12-07 11:38:12,048 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1745546438-127.0.1.1-1544149999758:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-12-07 11:42:29,392 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1745546438-127.0.1.1-1544149999758:blk_1073741826_1002 src: /127.0.0.1:60424 dest: /127.0.0.1:50010
2018-12-07 11:42:29,427 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:60424, dest: /127.0.0.1:50010, bytes: 526, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_54613630_14, offset: 0, srvID: 04c48cfa-039b-4f57-89f2-3db9de8d6e4c, blockid: BP-1745546438-127.0.1.1-1544149999758:blk_1073741826_1002, duration: 34184488
2018-12-07 11:42:29,427 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1745546438-127.0.1.1-1544149999758:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
